---
title: "Comparing data.table reshape to duckdb and polars"
author: "Toby Hocking"
date: "2024-03-10"
categories: [tips, tutorials, developer, benchmarks]
draft: false
---

One element of [the NSF POSE grant for
`data.table`](https://rdatatable-community.github.io/The-Raft/posts/2023-10-15-intro_to_grant-toby_hocking/)
is to create benchmarks which can inform users about when `data.table`
could be more performant than similar software. Two examples of
similar software are duckdb and polars, which each provide in-memory
database operations. This post explores the differences in
computational requirements, and in functionality, for data reshaping
operations. 

# Terminology and functions in R, `data.table`, and SQL

Data reshaping means changing the shape of the data, in order to get
it into a more appropriate format, for learning/plotting/etc. In R we
use the terms "wide" (many columns, few rows) and "long" (few columns,
many rows) to describe the different data shapes (and these terms come
from `?stats::reshape`), wheras in SQL we use the terms "pivoted" and
"unpivoted" to describe these two table types.

| R table type | SQL table type | rows | columns |
|--------------|----------------|------|---------|
| tall         | unpivoted      | many | few     |
| wide         | pivoted        | few  | many    |

For the wide-to-tall reshape operation, `data.table` has `melt()` and
SQL has `UNPIVOT`; for the tall-to-wide reshape operation,
`data.table` has `dcast()` and SQL has `PIVOT`.

| Reshape operation | `data.table` function | SQL function |
|-------------------|-----------------------|--------------|
| Wide-to-long      | `melt`                | `UNPIVOT`    |
| Long-to-wide      | `dcast`               | `PIVOT`      |

# Wide-to-long operations

We begin with a discussion of wide-to-long reshape operations, also
known as unpivot in SQL.

## Wide-to-long data reshape (unpivot) using `data.table::melt`

Wide-to-long reshape is often necessary before plotting. It is
perhaps best explained using a simple example. Here we consider the
iris data, which has four numeric columns:

```{r}
library(data.table)
(iris.wide <- data.table(iris))
```

What if we wanted to make a facetted histogram of the numeric iris
data columns, with one panel/facet for each column? With ggplots we
would use `geom_histogram(aes(numeric_variable))`, where
`numeric_variable` would be the column name of a data table containing
all of the numbers that we want to show in the histogram. To construct
that table, we would have to first reshape to "long" (or unpivoted)
format.  To easily understand what the reshape operation does, we show
a subset of the data (first and last rows) below:

```{r}
(two.iris.wide <- iris.wide[c(1,.N)])
```

Note the table above has 8 numbers, arranged into a table of 2 rows
and 4 columns. To reshape these data to "long" (or unpivoted) format,
we can use `data.table::melt`, as in the code below.

```{r}
melt(two.iris.wide, measure.vars=measure(part, dim, sep="."))
```

Note the table above has the same 8 numbers, but arranged into 1
column in a table with 8 rows, which is the desired input for ggplots.
Also note that the reshaped column names (`Petal.Length`,
`Sepal.Width`, etc) each consist of two components, which become two
different columns in the output: `part` (`Sepal` or `Petal`) and `dim`
(`Length` or `Width`). In the code above, we used `sep="."` to specify
that we want to split all of the iris column names using a dot, and
then reshape all of the columns whose names split into the max number
of items. The corresponding column names of the output are specified
as the arguments of `measure()`, and for more info about this
functionality, please read [its man
page](https://rdatatable.gitlab.io/data.table/reference/measure.html).

Below we do the same reshape with the full iris data set, this time
using a regular expression (instead of the `sep` argument used above),
```{r}
(iris.long <- melt(iris.wide, measure.vars=measure(part, dim, pattern="(.*)[.](.*)")))
```

In the code above, the `pattern` argument is a Perl-compatible regular expression, and columns that match the pattern will be reshaped. The pattern must contain the same number of capture groups (parentheses) as the number of other arguments to melt (part and dim), which are used for output column names. 
After reshaping, we plot the data in a histogram:

```{r dcast-hist}
library(ggplot2)
ggplot()+
  geom_histogram(aes(
    value),
    bins=50,
    data=iris.long)+
  facet_grid(part ~ dim, labeller=label_both)
```

We can see in the plot above that there is a top strip for each `dim`
and a right strip for each `part`, and each facet/panel contains a
histogram of the corresponding subset of data.

## Wide-to-long reshape via unpivot in polars

`polars` is an implementation of data frames in Rust, with bindings in R
and Python. In `polars`, the wide-to-long data reshape operation is
documented on the [man page for
unpivot](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.unpivot.html#polars.DataFrame.unpivot), which explains that we must specify `index` and/or `on`  (no support for separator, nor regex). In
our case, we use the code below:

```{r}
(iris.long.polars <- polars::as_polars_df(iris)$unpivot(
  index="Species",
  on=c("Sepal.Length","Petal.Length","Sepal.Width","Petal.Width"),
  variable_name="part.dim",
  value_name="cm"))
```

The output above is analogous to the result from `data.table::melt`,
but with one column named `part.dim` instead of the two columns named
`part` and `dim`, because polars does not support splitting the
reshaped column names into more than one output column. So with
polars, if we wanted separate `part` and `dim` columns, we would have
to specify that in a separate step, after the reshape. Or we could
just use `facet_wrap` instead of `facet_grid`, as in the code below:

```{r polars-hist}
ggplot()+
  geom_histogram(aes(
    cm),
    bins=50,
    data=iris.long.polars)+
  facet_wrap(. ~ part.dim, labeller=label_both)
```

We can see in the plot above that there is a facet for each of the
variables, but only one `part.dim` strip for each, instead of two strips
(`part` and `dim`), as was the case for the previous plot.

### Wide-to-long reshape via UNPIVOT in duckdb

`duckdb` is a column-oriented database implemented in C++, with an R
package that supports a DBI-complient SQL interface. That means that
we use R functions like `DBI::dbGetQuery` to get results, just like we
would with any other database (Postgres, MySQL, etc). This is
documented in the [duckdb R API](https://duckdb.org/docs/api/r.html)
docs, which explain how to create a database connection, and then copy
data from R to the database, as in the code below,

```{r}
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = ":memory:")
DBI::dbWriteTable(con, "iris_wide", iris)
```

The [duckdb unpivot man
page](https://duckdb.org/docs/sql/statements/unpivot.html) explains
how to do wide-to-tall reshape operations, which requires specifying names of columns to reshape (no support for separator, nor regex).  In our case, we use the
code below:

```{r}
iris.long.duckdb <- DBI::dbGetQuery(con, '
UNPIVOT iris_wide
ON "Sepal.Length", "Petal.Length", "Sepal.Width", "Petal.Width" 
INTO NAME part_dim 
VALUE cm')
str(iris.long.duckdb)
```

Above we use `str` to show a brief summary of the structure of the
output, which is a `data.frame` with 600 rows.
With duckbdb, the output has one column named `part_dim` (dots in column names are not allowed so we use an underscore here instead), because it does not support splitting the reshaped column names into more than one output column. So with duckdb, if we wanted separate `part` and `dim` columns, we would have to specify that in a separate step, after the reshape. 

## Creating `part` and `dim` columns

Both `polars` and `duckdb` are not capable of producing the separate
`part` and `dim` columns during the reshape operation, but we can
always do it as a post-processing step. One way to do that, by
specifying a separator, would be via `data.table::tstrsplit`, as in
the code below:

```{r}
data.table(iris.long.duckdb)[
, c("part","dim") := tstrsplit(part_dim, split="[.]")
][]
```

Another way of doing that, by specifying a regex, would be via
`nc::capture_first_df` (recently given the `data.table` [Seal of
Approval](https://rdatatable-community.github.io/The-Raft/posts/2024-08-01-seal_of_approval-nc/)),
as in the code below:

```{r}
nc::capture_first_df(iris.long.duckdb, part_dim=list(
  part=".*",
  "[.]",
  dim=".*"))
```

Both results above are data tables with extra cols `part` and `dim`. For visualization, these data tables could be used with either `facet_grid` or `facet_wrap`, similar to the examples above.

## Reshape into multiple columns

Another kind of wide-to-long reshape involves reshaping into multiple columns. 
For example, in the iris data, we may wonder whether sepals are larger than petals (in terms of both length and width). 
To answer that question, we could make a scatterplot of `y=Sepal` versus `x=Petal`, with a facet/panel for each dimension (`Length` and `Width`). 
In the ggplot system, we would need to compute a data table with columns `Sepal`, `Petal`, and `dim`, and we can do that by specifying the `value.name` keyword to `measure()`, as in the code below:

```{r}
(iris.long.parts <- melt(iris.wide, measure.vars=measure(value.name, dim, sep=".")))
```

Again, the `measure()` function in the code above operates by
splitting the input column names using `sep`, which results in two
groups (`Sepal` and `Width`, etc) for each of the measured
columns. The `value.name` keyword indicates that each unique value in
the first group (`Sepal` and `Petal`) should be used as the name of an
output column.
This functionality can be very convenient for some data reshaping tasks, but it is neither supported `polars`, nor in `duckdb`. 
Going back to our original motivating problem, we can make the scatterplot using the code below,

```{r iris-scatter}
ggplot()+
  theme_bw()+
  geom_abline(slope=1, intercept=0, color="grey")+
  geom_point(aes(
    Petal, Sepal),
    data=iris.long.parts)+
  facet_grid(. ~ dim, labeller=label_both)+
  coord_equal()
```

From the plot above, we see that since all of the data points (black) are above the y=x line (grey), so we can conclude that sepals are indeed larger than petals, in terms of both length and width.

## Wide-to-long performance comparison

We may also wonder which data reshaping functions work fastest for
large data. To answer that question, we will use `atime`, which is an
R package that allows us to see how much time/memory is required for
computations in R, as a function of data size `N`. In the `setup`
argument of the code below, we repeat the iris data for a certain
number of rows `N`. The code in the other arguments is run for the time/memory measurement, and is very similar to the code presented in previous sections. One difference is that for `data.table` we use `id.vars` instead of `measure()`, to more closely match the arguments provided to the other unpivot functions.

```{r}
seconds.limit <- 0.1
unpivot.res <- atime::atime(
  N=2^seq(1,50),
  setup={
    (row.id.vec <- 1+(seq(0,N-1) %% nrow(iris)))
    N.df <- iris[row.id.vec,]
    N.dt <- data.table(N.df)
    polars_df <- polars::as_polars_df(N.df)
    duckdb::dbWriteTable(con, "iris_table", N.df, overwrite=TRUE)
  },
  seconds.limit=seconds.limit,
  "duckdb\nUNPIVOT"=DBI::dbGetQuery(con, 'UNPIVOT iris_table ON "Sepal.Length", "Petal.Length", "Sepal.Width", "Petal.Width" INTO NAME part_dim VALUE cm'),
  "polars\nunpivot"=polars_df$unpivot(index="Species", value_name="cm"),
  "data.table\nmelt"=melt(N.dt, id.vars="Species", value.name="cm"))
unpivot.refs <- atime::references_best(unpivot.res)
unpivot.pred <- predict(unpivot.refs, seconds=seconds.limit, kilobytes=1000)
plot(unpivot.pred)+coord_cartesian(xlim=c(1e1,1e7))
```

In the plot above we see there are some differences:
* In the top panel, the memory usage in kilobytes is plotted as a function of `N`, the number of input rows to reshape. The horizontal reference line is drawn at 1000kb, and the `N` highlighted for `duckdb` and `data.table` are the number of rows that can be processed within that memory budget. We see that `data.table` is slightly more efficient than `duckdb`, and that `polars` is constant (polars/rust memory allocation is not measurable by `atime`).
* In the bottom panel, the computation time in seconds is plotted as a function of `N`, the number of input rows to reshape. The horizontal reference line is drawn at `r seconds.limit` seconds, and the `N` highlighted corresponds to the throughput given that time limit. We see that `data.table` is slightly faster than `polars` (within 2x), and both are much faster than `duckdb` (more than 10x).

Above there are several confounding factors in the comparison, most notably that data must be copied to `duckdb` and `polars` before and after processing. In contrast, `data.table` provides `setDT` and `setDF` functions, which can convert to/from data tables, without copying. So when data originates in R, or needs to come back to R, we should include the copy time for a more fair comparison. Below we run that comparison:

```{r}
seconds.limit <- 0.1
unpivot.copy.res <- atime::atime(
  N=2^seq(1,50),
  setup={
    (row.id.vec <- 1+(seq(0,N-1) %% nrow(iris)))
    N.df <- iris[row.id.vec,]
  },
  seconds.limit=seconds.limit,
  "duckdb\ncopy+UNPIVOT"={
    duckdb::dbWriteTable(con, "iris_table", N.df, overwrite=TRUE)
    DBI::dbGetQuery(con, 'UNPIVOT iris_table ON "Sepal.Length", "Petal.Length", "Sepal.Width", "Petal.Width" INTO NAME part_dim VALUE cm')
  },
  "polars\ncopy+unpivot"={
    polars_df <- polars::as_polars_df(N.df)
    polars_unpivot <- polars_df$unpivot(index="Species", value_name="cm")
    as.data.frame(polars_unpivot)
  },
  "data.table\nset+melt"=setDF(melt(setDT(N.df), id.vars="Species", value.name="cm")))
unpivot.copy.refs <- atime::references_best(unpivot.copy.res)
unpivot.copy.pred <- predict(unpivot.copy.refs, seconds=seconds.limit, kilobytes=500)
plot(unpivot.copy.pred)+coord_cartesian(xlim=c(1e1,1e7))
```

The result above again shows that `data.table` is most efficient in
terms of both time and memory. In this comparison, `data.table` is
faster than `polars` by a much larger amount (more than 5x).

## Wide-to-long summary

In this section, we showed that `data.table` provides an efficient and feature-rich implementation of wide-to-long data reshaping. 
* `measure()` allows specification of columns to reshape using either
  a separator or a regular expression pattern. In contrast, `duckdb`
  nor `polars` require specifying input column names (no support for
  separator, nor regex), and output column post-processing, which is
  less convenient.
* The `value.name` keyword can be used to reshape into multiple output
  columns, which is required for some kinds of reshape operations (no
  way to do that in `duckdb`/`polars`).
* `setDT` and `setDF` can be used to avoid un-necessary copies with `data.table`. In contrast, `duckdb`/`polars` require copies to/from regular R memory, which can add significant time/memory requirements.
* `data.table` was fastest and most memory efficient in the comparisons we examined (both with and without consideration of copying).

| feature          | `data.table` | `polars` | `duckdb` |
|------------------|--------------|----------|----------|
| sep/regex        | yes          | no       | no       |
| multiple outputs | yes          | no       | no       |
| avoid copies     | yes          | no       | no       |

# Long-to-wide

## Long-to-wide data reshape using `data.table::dcast`

## Similar long-to-wide reshape operations in polars and duckdb

UPDATE 26 sept 2024 added polars row based on docs for
[pivot](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.pivot.html)
and


 [pivot](https://duckdb.org/docs/sql/statements/pivot.html),Note that duckdb supports multiple value columns, and multiple aggregation functions, but you need to specify each combination of column/aggregation in a separate entry of `USING`, so this is less convenient than `data.table::dcast`, which computes each aggregation function for each value column.


## Conclusion

We have discussed the plan for augmenting the testing infrastructure available for `data.table` (performance testing, benchmarking, and revdep checking). Hopefully the new testing infrastructure will allow contributors to be more confident about merging PRs with bug fixes and new features.

## Attribution

Parts of this blog post were copied from [my more extensive comparison
blog](https://tdhock.github.io/blog/2024/collapse-reshape/).

## Session info

```{r}
sessionInfo()
```
