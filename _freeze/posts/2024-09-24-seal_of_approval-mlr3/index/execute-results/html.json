{
  "hash": "5ab16e6920712622215e3fad7a688b92",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seal of Approval: mlr3\"\nauthor: \"Maximilian Mücke\"\ndate: \"Sep 24, 2024\"\ncategories: [seal of approval, application package]\nimage: \"hex_approved.png\"\ndraft: true\n---\n\n\n## [`mlr3`](https://mlr3.mlr-org.com/)\n\n*Author(s):* Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder, Florian Pfisterer, Raphael Sonabend, Marc Becker, Sebastian Fischer\n\n*Maintainer:* Marc Becker (marcbecker\\@posteo.de)\n\n[Seal of Approval](https://github.com/Rdatatable/data.table/pull/6430)\n\n![mlr3 hex sticker](hex.png)\n\nA modern object-oriented machine learning framework. Successor of [mlr](https://github.com/mlr-org/mlr).\n\n## Relationship with `data.table`\n\n`mlr3` was designed to integrate closely with `data.table` for efficient data handling in machine learning workflows. There are two main ways `mlr3` is related to `data.table`:\n\n1.  **Data Backend**: `mlr3` uses `data.table` as the core data backend for all `Task` objects. This means that when you work with tasks in `mlr3`, the underlying data is stored and managed using `data.table`. Moreover, users can leverage `data.table` syntax directly within `mlr3` workflows. Accessing task data via `task$data()` returns a `data.table`, enabling you to apply `data.table` operations for data preprocessing, feature engineering, and subsetting without any additional conversion or overhead.\n2.  **Result Storage**: `mlr3` stores various results such as predictions, resampling outcomes, and benchmarking results as `data.table` objects.\n\n## Overview\n\n*Excerpted from the [`mlr3` book](https://mlr3book.mlr-org.com/chapters/chapter1/introduction_and_overview.html)*\n\nThe mlr3 universe includes a wide range of tools taking you from basic ML to complex experiments. To get started, here is an example of the simplest functionality – training a model and making predictions.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\n\ntask = tsk(\"penguins\")\nsplit = partition(task)\nlearner = lrn(\"classif.rpart\")\n\nlearner$train(task, row_ids = split$train)\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 230 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 230 128 Adelie (0.443478261 0.195652174 0.360869565)  \n  2) flipper_length< 205 144  42 Adelie (0.708333333 0.284722222 0.006944444)  \n    4) bill_length< 43.35 102   3 Adelie (0.970588235 0.029411765 0.000000000) *\n    5) bill_length>=43.35 42   4 Chinstrap (0.071428571 0.904761905 0.023809524) *\n  3) flipper_length>=205 86   4 Gentoo (0.000000000 0.046511628 0.953488372) *\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 114 observations:\n    row_ids     truth  response\n          2    Adelie    Adelie\n          3    Adelie    Adelie\n          4    Adelie    Adelie\n---                            \n        336 Chinstrap Chinstrap\n        337 Chinstrap    Gentoo\n        344 Chinstrap Chinstrap\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n  0.9122807 \n```\n\n\n:::\n:::\n\n\nIn this example, we trained a decision tree on a subset of the `penguins` dataset, made predictions on the rest of the data and then evaluated these with the accuracy measure.\n\nThe `mlr3` interface also lets you run more complicated experiments in just a few lines of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\ntasks = tsks(c(\"german_credit\", \"sonar\"))\n\nglrn_rf_tuned = as_learner(ppl(\"robustify\") %>>% auto_tuner(\n    tnr(\"grid_search\", resolution = 5),\n    lrn(\"classif.ranger\", num.trees = to_tune(200, 500)),\n    rsmp(\"holdout\")\n))\nglrn_rf_tuned$id = \"RF\"\n\nglrn_stack = as_learner(ppl(\"robustify\") %>>% ppl(\"stacking\",\n    lrns(c(\"classif.rpart\", \"classif.kknn\")),\n    lrn(\"classif.log_reg\")\n))\nglrn_stack$id = \"Stack\"\n\nlearners = c(glrn_rf_tuned, glrn_stack)\nbmr = benchmark(benchmark_grid(tasks, learners, rsmp(\"cv\", folds = 3)))\n\nbmr$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr       task_id learner_id resampling_id iters classif.acc\n   <int>        <char>     <char>        <char> <int>       <num>\n1:     1 german_credit         RF            cv     3   0.7749966\n2:     2 german_credit      Stack            cv     3   0.7450175\n3:     3         sonar         RF            cv     3   0.8077295\n4:     4         sonar      Stack            cv     3   0.7121463\nHidden columns: resample_result\n```\n\n\n:::\n:::\n\n\nIn this more complex example, we selected two tasks and two learners, used automated tuning to optimize the number of trees in the random forest learner, and employed a machine learning pipeline that imputes missing data, consolidates factor levels, and stacks models. We also showed basic features like loading learners and choosing resampling strategies for benchmarking. Finally, we compared the performance of the models using the mean accuracy with three-fold cross-validation.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}