{
  "hash": "9de9900470096c72dc265aeeb50ceb03",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seal of Approval: mlr3\"\nauthor: \"Maximilian Mücke\"\ndate: \"Oct 01, 2024\"\ncategories: [seal of approval, application package]\nimage: \"hex_approved.png\"\ndraft: false\n---\n\n\n\n\n## [`mlr3`](https://mlr3.mlr-org.com/)\n\n*Author(s):* Michel Lang, Bernd Bischl, Jakob Richter, Patrick Schratz, Martin Binder, Florian Pfisterer, Raphael Sonabend, Marc Becker, Sebastian Fischer\n\n*Maintainer:* Marc Becker (marcbecker\\@posteo.de)\n\n[Seal of Approval](https://github.com/Rdatatable/data.table/pull/6430)\n\n![mlr3 hex sticker](hex.png)\n\nA modern object-oriented machine learning framework. Successor of [mlr](https://github.com/mlr-org/mlr).\n\n## Relationship with `data.table`\n\n`mlr3` was designed to integrate closely with `data.table` for efficient data handling in machine learning workflows. There are two main ways `mlr3` is related to `data.table`:\n\n1.  **Data Backend**: `mlr3` uses `data.table` as the core data backend for all `Task` objects. This means that when you work with tasks in `mlr3`, the underlying data is stored and managed using `data.table`. Moreover, users can leverage `data.table` syntax directly within `mlr3` workflows. Accessing task data via `task$data()` returns a `data.table`, enabling you to apply `data.table` operations for data preprocessing, feature engineering, and subsetting without any additional conversion or overhead.\n2.  **Result Storage**: `mlr3` stores various results such as predictions, resampling outcomes, and benchmarking results as `data.table` objects.\n\n## Overview\n\n*Excerpted from the [`mlr3` book](https://mlr3book.mlr-org.com/chapters/chapter1/introduction_and_overview.html)*\n\nThe mlr3 universe includes a wide range of tools taking you from basic ML to complex experiments. To get started, here is an example of the simplest functionality – training a model and making predictions.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\n\ntask = tsk(\"penguins\")\nsplit = partition(task)\nlearner = lrn(\"classif.rpart\")\n\nlearner$train(task, row_ids = split$train)\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 230 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 230 129 Adelie (0.439130435 0.186956522 0.373913043)  \n  2) flipper_length< 206.5 140  41 Adelie (0.707142857 0.285714286 0.007142857)  \n    4) bill_length< 43.05 98   3 Adelie (0.969387755 0.030612245 0.000000000) *\n    5) bill_length>=43.05 42   5 Chinstrap (0.095238095 0.880952381 0.023809524) *\n  3) flipper_length>=206.5 90   5 Gentoo (0.022222222 0.033333333 0.944444444) *\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 114 observations:\n row_ids     truth  response\n       3    Adelie    Adelie\n       5    Adelie    Adelie\n       6    Adelie    Adelie\n     ---       ---       ---\n     342 Chinstrap Chinstrap\n     343 Chinstrap    Gentoo\n     344 Chinstrap Chinstrap\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n  0.9473684 \n```\n\n\n:::\n:::\n\n\n\n\nIn this example, we trained a decision tree on a subset of the `penguins` dataset, made predictions on the rest of the data and then evaluated these with the accuracy measure.\n\nThe `mlr3` interface also lets you run more complicated experiments in just a few lines of code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\ntasks = tsks(c(\"german_credit\", \"sonar\"))\n\nglrn_rf_tuned = as_learner(ppl(\"robustify\") %>>% auto_tuner(\n    tnr(\"grid_search\", resolution = 5),\n    lrn(\"classif.ranger\", num.trees = to_tune(200, 500)),\n    rsmp(\"holdout\")\n))\nglrn_rf_tuned$id = \"RF\"\n\nglrn_stack = as_learner(ppl(\"robustify\") %>>% ppl(\"stacking\",\n    lrns(c(\"classif.rpart\", \"classif.kknn\")),\n    lrn(\"classif.log_reg\")\n))\nglrn_stack$id = \"Stack\"\n\nlearners = c(glrn_rf_tuned, glrn_stack)\nbmr = benchmark(benchmark_grid(tasks, learners, rsmp(\"cv\", folds = 3)))\n\nbmr$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr       task_id learner_id resampling_id iters classif.acc\n   <int>        <char>     <char>        <char> <int>       <num>\n1:     1 german_credit         RF            cv     3   0.7749966\n2:     2 german_credit      Stack            cv     3   0.7450175\n3:     3         sonar         RF            cv     3   0.8077295\n4:     4         sonar      Stack            cv     3   0.7121463\nHidden columns: resample_result\n```\n\n\n:::\n:::\n\n\n\n\nIn this more complex example, we selected two tasks and two learners, used automated tuning to optimize the number of trees in the random forest learner, and employed a machine learning pipeline that imputes missing data, consolidates factor levels, and stacks models. We also showed basic features like loading learners and choosing resampling strategies for benchmarking. Finally, we compared the performance of the models using the mean accuracy with three-fold cross-validation.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}